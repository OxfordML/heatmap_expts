'''
Plot the results of the simulations with increasing numbers of data points. Show accuracy etc. 

Currently using the results generated by expmt_module -- can alter to use the Ushahidi and Cicada data later.

Created on 23 Nov 2015

@author: edwin
'''

import numpy as np
import matplotlib.pyplot as plt
import logging, os

#import run_synthetic_bias as expmt_module
#import run_synthetic_noise as expmt_module
import run_synthetic_noise_nogrid as expmt_module
#import ushahidi_loader_damage as expmt_module
#import ushahidi_loader_emergencies as expmt_module
#import prn_simulation as expmt_module

if hasattr(expmt_module, 'cluster_spreads'):
    cluster_spreads = expmt_module.cluster_spreads
elif hasattr(expmt_module, 'featurenames'):
    cluster_spreads = [expmt_module.featurenames[0]]
    print cluster_spreads
else:
    cluster_spreads = [0]

nruns = expmt_module.nruns
if hasattr(expmt_module, 'weak_proportions'):
    weak_proportions = expmt_module.weak_proportions
else:
    weak_proportions = [-1]

def get_data_dir(d, p, p_idx, cluster_spread):
    if p==-1:
        dataset_label = "d%i" % d # no proportion indices
    else:
        dataset_label = "p%f_d%i" % (p, d)
    
    expt_label = expmt_module.expt_label_template
    if '%' in expt_label:
        expt_label = expt_label % cluster_spread
         
    logging.info("Loading results for proportion %f, Dataset %d, cluster spread %s" % (p, d, str(cluster_spread)))
    datadir, _ = expmt_module.dataset_location(expt_label, dataset_label)
    return datadir

def get_output_dir(p, cluster_spread=0):
    expt_label = expmt_module.expt_label_template
    
    if p==-1:
        dataset_label = "" # no proportion indices
    else:
        dataset_label = "p%.3f" % p
    
    if '%' in expt_label:
        expt_label = expt_label % cluster_spread
        
    outputdir =  './output/' 
    if not os.path.isdir(outputdir):
        os.mkdir(outputdir)
    if 'synth/' in expt_label and not os.path.isdir(outputdir + '/synth/'):
        os.mkdir(outputdir + '/synth/')        
    if 'prn4/' in expt_label and not os.path.isdir(outputdir + '/prn4/'):
        os.mkdir(outputdir + '/prn4/')        
    outputdir += expt_label + '/'
    if not os.path.isdir(outputdir):
        os.mkdir(outputdir) 
    outputdir += dataset_label
    if not os.path.isdir(outputdir):
        os.mkdir(outputdir)
    print "Using output dir %s" % outputdir
    return outputdir

def load_median_results(nruns, weak_proportions, filename):
    median_results = {}
    lq_results = {}
    uq_results = {} 
    for p_idx, p in enumerate(weak_proportions):
        for cluster_spread in cluster_spreads:
            results_pcs = {}
            for d in range(nruns):
                datadir = get_data_dir(d, p, p_idx, cluster_spread)
    
                current_results = np.load(datadir + filename).item()
                methods = current_results.keys()
    
                if p not in median_results:
                    median_results[p] = {}
                    lq_results[p] = {}
                    uq_results[p] = {}
                    
                if cluster_spread not in median_results[p]:
                    median_results[p][cluster_spread] = {}
                    lq_results[p][cluster_spread] = {}
                    uq_results[p][cluster_spread] = {}                
                
                for m in methods:
                    if not m in results_pcs:
                        results_pcs[m] = np.zeros((nruns, len(Nreps_iter))) 
                        
                    if len(current_results[m])==24 and len(Nreps_iter)==11:
                        print len(current_results[m])
                        current_results[m] = np.array(current_results[m])[[0,2,4,6,8,10,12,14,16,18,20]]
                    results_pcs[m][d, :] = current_results[m]
        
            for m in results_pcs:
                median_results[p][cluster_spread][m] = np.median(results_pcs[m], axis=0)
                lq_results[p][cluster_spread][m] = np.percentile(results_pcs[m], 25, axis=0)
                uq_results[p][cluster_spread][m] = np.percentile(results_pcs[m], 75, axis=0)
                
    return median_results, lq_results, uq_results, methods

def load_diff_results(nruns, weak_proportions, filename, negate_diff=False):
    median_results = {}
    lq_results = {}
    uq_results = {}
    for p_idx, p in enumerate(weak_proportions):
        for cluster_spread in cluster_spreads:
            results_pcs = {}
            for d in range(nruns):
                datadir = get_data_dir(d, p, p_idx, cluster_spread)
    
                current_results = np.load(datadir + filename).item()
                methods = current_results.keys()
                
                testmethod = 'HeatmapBCC' #compare this against the
    
                if p not in median_results:
                    median_results[p] = {}
                    lq_results[p] = {}
                    uq_results[p] = {}
                    
                if cluster_spread not in median_results[p]:
                    median_results[p][cluster_spread] = {}
                    uq_results[p][cluster_spread] = {}
                    lq_results[p][cluster_spread] = {}                
                
                testresults = np.array(current_results[testmethod])
                if len(testresults)==24 and len(Nreps_iter)==11:
                    testresults = np.array(testresults)[[0,2,4,6,8,10,12,14,16,18,20]]
                
                for m in methods:
                    if not m in results_pcs:
                        results_pcs[m] = np.zeros((nruns, len(Nreps_iter))) 
                        
                    if len(current_results[m])==24 and len(Nreps_iter)==11:
                        print len(current_results[m])
                        current_results[m] = np.array(current_results[m])[[0,2,4,6,8,10,12,14,16,18,20]]
                    
                    if not m==testmethod:
                        current_results[m] = testresults - np.array(current_results[m])
                        if negate_diff:
                            current_results[m] = - current_results[m]
                        results_pcs[m][d, :] = current_results[m]
        
            for m in results_pcs:
                median_results[p][cluster_spread][m] = np.median(results_pcs[m], axis=0)
                lq_results[p][cluster_spread][m] = np.percentile(results_pcs[m], 25, axis=0)
                uq_results[p][cluster_spread][m] = np.percentile(results_pcs[m], 75, axis=0)
    return median_results, lq_results, uq_results, methods

if __name__ == '__main__':
    if hasattr(expmt_module, 'Nreports'):
        Nreports = expmt_module.Nreports
    else:
        _, Nreports, _, _, _, _ = expmt_module.load_data()
    
    # import settings from where the experiments were run
    if hasattr(expmt_module, "Nreps_initial"):
        Nreps_initial = expmt_module.Nreps_initial
    else:
        Nreps_initial = expmt_module.Nreps_initial_fraction * Nreports
        
    if Nreps_initial < 1:
        Nreps_initial = Nreps_initial * Nreports # Nreps_initial is the initial fraction
        
    if hasattr(expmt_module, 'Nrep_inc'):
        Nrep_inc = expmt_module.Nrep_inc
    else:
        Nrep_inc = (Nreports - Nreps_initial) / (expmt_module.nsteps - 1) 
          
    if hasattr(expmt_module, 'nsteps'):
        Nsteps = expmt_module.nsteps
    else:
        Nsteps = (Nreports - Nreps_initial) / float(Nrep_inc) + 1
    
    # get a set of x-coordinates for the number of reports at each iteration
    Nreps_iter = np.arange(Nsteps) * Nrep_inc + Nreps_initial
    
    colors = {'HeatmapBCC':'g',
              'IBCC':'r',
              'IBCC+GP':'purple',
              'GP':'saddlebrown',
              'KDE':'b'}
    
    marks = {'HeatmapBCC':'x',
              'IBCC':'^',
              'IBCC+GP':'o',
              'GP':'v',
              'KDE':'+'}    
    
    # load results for the density estimation        
    # Root mean squared error
    rmsed, rmsed_lq, rmsed_uq, methods = load_median_results(nruns, weak_proportions, "rmsed.npy")
         
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:
            plt.figure()
            plt.title('Root Mean Square Error of Density Estimates')
            for i, m in enumerate(methods):
                label = m
                if len(rmsed[p][cs][m])==24 and len(Nreps_iter)==11:
                    rmsed[p][cs][m] = rmsed[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]
                plt.plot(Nreps_iter, rmsed[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))            
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('RMSE')
            #plt.ylim(0, 1.0)
            plt.legend(loc='best')
            plt.grid(True)
            for i, m in enumerate(methods):
                plt.fill_between(Nreps_iter, rmsed_lq[p][cs][m], rmsed_uq[p][cs][m], 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/rmsed.pdf")
      
#     # Kendall's Tau
#     tau, tau_quartile, methods = load_median_results(nruns, weak_proportions, "tau.npy")
#   
#     for p_idx, p in enumerate(weak_proportions):
#         for cs in cluster_spreads:        
#             plt.figure()
#             plt.title("Kendall's Tau for Density Estimates")
#             for i, m in enumerate(methods):
#                 label = m
#                 if len(tau[p][cs][m])==24 and len(Nreps_iter)==11:
#                     tau[p][cs][m] = tau[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]
#                 plt.plot(Nreps_iter, tau[p][cs][m], label=label, color=colors[m], marker=marks[m])
#             plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))                
#             plt.xlabel('Number of crowdsourced labels')
#             plt.ylabel('tau')
#             #plt.ylim(-1.0, 1.0)
#             plt.legend(loc='best')
#             plt.grid(True)
#        
#             for i, m in enumerate(methods):
#                 plt.fill_between(Nreps_iter, tau[p][cs][m] - tau_quartile[p][cs][m], tau[p][cs][m] + tau_quartile[p][cs][m], 
#                                  alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
#             
#             outputdir = get_output_dir(p, cs)
#             plt.savefig(outputdir + "/tau.pdf")       
# 
#     # Kendall's Tau
#     tau, tau_quartile, methods = load_diff_results(nruns, weak_proportions, "tau.npy")
#   
#     for p_idx, p in enumerate(weak_proportions):
#         for cs in cluster_spreads:        
#             plt.figure()
#             plt.title("HeatmapBCC Improvement in \n Kendall's Tau for Density Estimates")
#             for i, m in enumerate(methods):
#                 label = m
#                 
#                 if m=='HeatmapBCC':
#                     continue       
#                 
#                 if len(tau[p][cs][m])==24 and len(Nreps_iter)==11:
#                     tau[p][cs][m] = tau[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]
#                 plt.plot(Nreps_iter, tau[p][cs][m], label=label, color=colors[m], marker=marks[m])
#             plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))                
#             plt.xlabel('Number of crowdsourced labels')
#             plt.ylabel('tau')
#             #plt.ylim(-1.0, 1.0)
#             plt.legend(loc='best')
#             plt.grid(True)
#        
#             for i, m in enumerate(methods):
#                 plt.fill_between(Nreps_iter, tau[p][cs][m] - tau_quartile[p][cs][m], tau[p][cs][m] + tau_quartile[p][cs][m], 
#                                  alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
#             
#             outputdir = get_output_dir(p, cs)
#             plt.savefig(outputdir + "/tau_diff.pdf")   
#        
    # Mean Cross Entropy
    mced, mced_lq, mced_uq, methods = load_median_results(nruns, weak_proportions, "mced.npy")
    methods = ["KDE", "GP", "IBCC+GP", "IBCC", "HeatmapBCC"]
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Negative Log Probability Density of Density Estimates")
            for i, m in enumerate(methods):
                label = m
                 
                if not m in mced[p][cs]:
                    continue
                 
                if len(mced[p][cs][m])==24 and len(Nreps_iter)==11:
                    mced[p][cs][m] = mced[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                
                plt.plot(Nreps_iter, np.log2(np.e) * mced[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('NLPD or Cross Entropy (bits)')
            plt.legend(loc='best')
            plt.grid(True)
             
            for i, m in enumerate(methods):
                if not m in mced[p][cs]:
                    continue
                plt.fill_between(Nreps_iter, np.log2(np.e) * (mced_lq[p][cs][m]), 
                                 np.log2(np.e) * (mced_uq[p][cs][m]), 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
             
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/mce_density.pdf")            
 
    # Mean Cross Entropy
    mced, mced_lq, mced_uq, methods = load_diff_results(nruns, weak_proportions, "mced.npy", negate_diff=True)
    methods = ["KDE", "GP", "IBCC+GP", "IBCC", "HeatmapBCC"]
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Improvement of HeatmapBCC in \n Negative Log Probability Density of Density Estimates")
            for i, m in enumerate(methods):
                label = m
                 
                if m=='HeatmapBCC':
                    continue    
                 
                if not m in mced[p][cs]:
                    continue            
                 
                if len(mced[p][cs][m])==24 and len(Nreps_iter)==11:
                    mced[p][cs][m] = mced[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                
                plt.plot(Nreps_iter, np.log2(np.e) * mced[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('NLPD or Cross Entropy (bits)')
            plt.legend(loc='best')
            plt.grid(True)
             
            for i, m in enumerate(methods):
                if not m in mced[p][cs]:
                    continue                
                plt.fill_between(Nreps_iter, np.log2(np.e) * (mced_lq[p][cs][m]), 
                                 np.log2(np.e) * (mced_uq[p][cs][m]), 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
             
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/mce_density_diff.pdf")
     
    # load results for predicting individual data points
    # Brier score
    rmse, rmse_lq, rmse_uq, methods = load_median_results(nruns, weak_proportions, "rmse.npy")   
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Brier Score")
            for i, m in enumerate(methods):
                label = m
                if len(rmse[p][cs][m])==24 and len(Nreps_iter)==11:
                    rmse[p][cs][m] = rmse[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                    
                plt.plot(Nreps_iter, rmse[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('Brier Score')
            #plt.ylim(0, 1.0)
            plt.legend(loc='best')
            plt.grid(True)
            
            for i, m in enumerate(methods):
                plt.fill_between(Nreps_iter, rmse_lq[p][cs][m], rmse_uq[p][cs][m], 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])
        
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/brier.pdf") 
         
    # AUC
    auc, auc_lq, auc_uq, methods = load_median_results(nruns, weak_proportions, "auc.npy")
    methods = ["HeatmapBCC", "IBCC", "IBCC+GP", "GP", "KDE"]
    for p in weak_proportions:
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("AUC")
            for i, m in enumerate(methods):
                label = m
                if not m in auc[p][cs]:
                    continue                    
                if len(auc[p][cs][m])==24 and len(Nreps_iter)==11:
                    auc[p][cs][m] = auc[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]             
                plt.plot(Nreps_iter, auc[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('AUC')
            #plt.ylim(0, 1.0)
            plt.legend(loc='best')
            plt.grid(True)

            for i, m in enumerate(methods):
                if not m in auc[p][cs]:
                    continue                    
                plt.fill_between(Nreps_iter, auc_lq[p][cs][m], auc_uq[p][cs][m], 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/auc.pdf")
     
    # Differences in AUC
    auc, auc_lq, auc_uq, methods = load_diff_results(nruns, weak_proportions, "auc.npy")
    methods = ["HeatmapBCC", "IBCC", "IBCC+GP", "GP", "KDE"]
    for p in weak_proportions:
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("AUC Improvement of HeatmapBCC")
            for i, m in enumerate(methods):
                label = m
                
                if m=='HeatmapBCC':
                    continue                
                if not m in auc[p][cs]:
                    continue                    
                if len(auc[p][cs][m])==24 and len(Nreps_iter)==11:
                    auc[p][cs][m] = auc[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]             
                plt.plot(Nreps_iter, auc[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('AUC')
            #plt.ylim(0, 1.0)
            plt.legend(loc='best')
            plt.grid(True)

            for i, m in enumerate(methods):
                if not m in auc[p][cs]:
                    continue                    
                plt.fill_between(Nreps_iter, auc_lq[p][cs][m], auc_uq[p][cs][m], 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/auc_diffs.pdf")        
     
    # Mean cross entropy
    cross_entropy, cross_entropy_lq, cross_entropy_uq, methods = load_median_results(nruns, weak_proportions, "mce.npy")
    methods = ["KDE", "GP", "IBCC+GP", "IBCC", "HeatmapBCC"]
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Cross Entropy (Negative Log Probability) of Classifications")
            for i, m in enumerate(methods):
                label = m
                if not m in cross_entropy[p][cs]:
                    continue                
                if len(cross_entropy[p][cs][m])==24 and len(Nreps_iter)==11:
                    cross_entropy[p][cs][m] = cross_entropy[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                    
                plt.plot(Nreps_iter, np.log2(np.e) * cross_entropy[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('Cross Entropy (bits)')
            plt.legend(loc='best')
            plt.grid(True)
            
            for i, m in enumerate(methods):
                if not m in cross_entropy[p][cs]:
                    continue                           
                plt.fill_between(Nreps_iter, np.log2(np.e) * (cross_entropy_lq[p][cs][m]),
                                 np.log2(np.e) * (cross_entropy_uq[p][cs][m]), 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/mce_discrete.pdf")
            print "saving to %s" % outputdir
            
    # Mean cross entropy difference
    cross_entropy, cross_entropy_lq, cross_entropy_uq, methods = load_diff_results(nruns, weak_proportions, "mce.npy", negate_diff=True)
    methods = ["KDE", "GP", "IBCC+GP", "IBCC", "HeatmapBCC"]
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Improvement of HeatmapBCC in \n Cross Entropy of Classifications")
            for i, m in enumerate(methods):
                
                if m=='HeatmapBCC':
                    continue
                if not m in cross_entropy[p][cs]:
                    continue
                label = m
                if len(cross_entropy[p][cs][m])==24 and len(Nreps_iter)==11:
                    cross_entropy[p][cs][m] = cross_entropy[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                    
                plt.plot(Nreps_iter, np.log2(np.e) * cross_entropy[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('Cross Entropy (bits)')
            plt.legend(loc='best')
            plt.grid(True)
            
            for i, m in enumerate(methods):
                if not m in cross_entropy[p][cs]:
                    continue                           
                plt.fill_between(Nreps_iter, np.log2(np.e) * (cross_entropy_lq[p][cs][m]),
                                 np.log2(np.e) * (cross_entropy_uq[p][cs][m]), 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/mce_discrete_diffs.pdf")
            print "saving to %s" % outputdir

    acc, acc_lq, acc_uq, methods = load_median_results(nruns, weak_proportions, "acc.npy")
    for p_idx, p in enumerate(weak_proportions):
        for cs in cluster_spreads:        
            plt.figure()
            plt.title("Classification Accuracy")
            for i, m in enumerate(methods):
                label = m
                if len(acc[p][cs][m])==24 and len(Nreps_iter)==11:
                    acc[p][cs][m] = acc[p][cs][m][[0,2,4,6,8,10,12,14,16,18,20]]                     
                plt.plot(Nreps_iter, acc[p][cs][m], label=label, color=colors[m], marker=marks[m])
            plt.xlim(np.min(Nreps_iter), np.max(Nreps_iter))
            plt.xlabel('Number of crowdsourced labels')
            plt.ylabel('Fraction of Points Correctly Classified')
            plt.legend(loc='best')
            plt.grid(True)
            
            for i, m in enumerate(methods):
                plt.fill_between(Nreps_iter, acc_lq[p][cs][m], acc_uq[p][cs][m], 
                                 alpha=0.1, edgecolor=colors[m], facecolor=colors[m])           
            
            outputdir = get_output_dir(p, cs)
            plt.savefig(outputdir + "/acc.pdf")
            print "saving to %s" % outputdir
                    
    # Variances within a single dataset
    #rmse_var = load_median_results(nruns, weak_proportions, "rmse_var.npy")
    #auc_var = load_median_results(nruns, weak_proportions, "auc_var.npy")
    #mce_var = load_median_results(nruns, weak_proportions, "mce_var.npy")    
